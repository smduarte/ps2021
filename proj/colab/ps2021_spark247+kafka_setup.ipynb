{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2021-spark247+kafka-setup.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1y2FYAxVQ3dtk8k4LwKtQTpNBTz3MegW7",
      "authorship_tag": "ABX9TyMyr73iQcbUQSS8KorPEDkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/ps2021/blob/master/proj/colab/ps2021_spark247%2Bkafka_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2021 \n",
        "## Google Colab Project Development Environment \n",
        "\n",
        "This notebook can be used to setup Spark, as well as the Kafka taxi data source for the first project.\n",
        "\n",
        "Compared to using Docker, the full taxi data can be kept\n",
        "remotely in Google Drive.\n",
        "\n",
        "Additionally, the Google Colab instances feature 12+GB of RAM and lots of disk space. \n",
        "\n",
        "The main drawback is the execution environment is not persistent. The notebook itself is saved in Google Drive, but the Spark and Kafka instalation needs to be repeated everytime you reopen the notebook. Fortunately, it only\n",
        "the procedure only takes a couple of minutes and it is fully automated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31XT-ghQ3Lca"
      },
      "source": [
        "### Mount a Google Drive\n",
        "\n",
        "Execute the following cell to mount a google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp7beUXtIuWQ"
      },
      "source": [
        "### Setup Spark and Kafka\n",
        "\n",
        "The command below will download and execute a script to setup spark and kafka as a local single machine cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9M-Bs99AdBz",
        "outputId": "6d4fa31c-31eb-4e5b-9352-28ac79bfbf88"
      },
      "source": [
        "!wget -q -O - https://raw.githubusercontent.com/smduarte/ps2021/master/proj/colab/setup-spark+kafka.sh  | bash \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Spark...\n",
            "Unpacking Spark...\n",
            "Downloading spark-sql-kafka-0-10_2.11-2.4.7.jar...\n",
            "Downloading spark-streaming-kafka-0-10-assembly_2.11-2.4.7.jar...\n",
            "Downloading spark-streaming-kafka-0-10_2.11-2.4.7.jar...\n",
            "Downloading spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar...\n",
            "Downloading spark-streaming-kafka-0-8_2.11-2.4.7.jar...\n",
            "Done\n",
            "Downloading Kafka...\n",
            "Unpacking Kafka...\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksWoYxjRJGh_"
      },
      "source": [
        "### Start Kafka \n",
        "\n",
        "Execute the cell below to start the Kafka source.\n",
        "\n",
        "`SPEEDUP` controls how fast the events are replayed relative to realtime. A value of 60 means that 1 second\n",
        "of realtime corresponds to 1 minute worth of events in the stream.\n",
        "\n",
        "`TAXI_DATA` should point to the compressed taxi data. You can use the sample dataset, or the full dataset. The\n",
        "full dataset can be stored in Google Drive used\n",
        "from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLZhbBuZEzj_",
        "outputId": "a9a21633-5767-41b8-d8d9-c5a8c6c8c949"
      },
      "source": [
        "%%bash\n",
        "\n",
        "SPEEDUP=60\n",
        "TAXI_DATA=/content/drive/MyDrive/PS2021/proj/sample.csv.gz\n",
        "\n",
        "wget -q -O - https://raw.githubusercontent.com/smduarte/ps2021/master/proj/colab/start-kafka.sh \\\n",
        "  | bash /dev/stdin $SPEEDUP $TAXI_DATA"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60 /content/drive/MyDrive/PS2021/proj/sample.csv.gz\n",
            "No zookeeper server to stop\n",
            "No kafka server to stop\n",
            "Starting Zookeeper...\n",
            "Starting Kafka...\n",
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5USgZw57dX"
      },
      "source": [
        "### Structured Spark Streaming\n",
        "\n",
        "The code below shows how to access the kafka stream in structured mode. Note that with this setup, we need to connect to `localhost`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpO0aX2PPWd1",
        "outputId": "b94a7267-9ded-4e03-da9f-887b9fa49a9f"
      },
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(20, False)\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Kafka Spark Structured Streaming Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(spark)\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "  .option(\"subscribe\", \"debs\") \\\n",
        "  .load() \\\n",
        "  .selectExpr(\"CAST(value AS STRING)\")\n",
        "\n",
        "split_lines = split(lines['value'], ',')\n",
        "\n",
        "rides = lines.withColumn('medallion', split_lines.getItem(0).cast(\"string\")) \\\n",
        "        .withColumn('pickup_datetime', split_lines.getItem(2).cast(\"timestamp\")) \\\n",
        "        .drop('value')\n",
        "\n",
        "query = rides \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination( 20)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f1b13187f50>\n",
            "+---------+---------------+\n",
            "|medallion|pickup_datetime|\n",
            "+---------+---------------+\n",
            "+---------+---------------+\n",
            "\n",
            "+--------------------------------+-------------------+\n",
            "|medallion                       |pickup_datetime    |\n",
            "+--------------------------------+-------------------+\n",
            "|019319FCEB17B9B9E45763F9573DE640|2013-01-01 00:00:19|\n",
            "|02B5097C51EBA10438C43AC0F4BC5867|2013-01-01 00:00:03|\n",
            "|02C49A409C2DC66B1DBD44A6EF1B75B5|2013-01-01 00:00:22|\n",
            "|02CDF4B814C18DE3898FDA761EA8A5B4|2013-01-01 00:00:26|\n",
            "|050E71DA009681B177455E36A1FDB041|2013-01-01 00:00:24|\n",
            "|067F347AA3A87A1F35E8EDC20083F43A|2013-01-01 00:00:19|\n",
            "|06F02AE93C005820592CC14E6E6E9B51|2013-01-01 00:00:19|\n",
            "|079E2EE80E3CFA17EB826FDF4DF66C36|2013-01-01 00:00:22|\n",
            "|0812169BAD07C1760F17DA2104920BF1|2013-01-01 00:00:17|\n",
            "|0BC78FA149D8D784815895BD33F17EE1|2013-01-01 00:00:17|\n",
            "|0C52944B57BE5E0962C6561A45C67A4A|2013-01-01 00:00:16|\n",
            "|0CE8A32DD9ED7C681471A968C71F31D9|2013-01-01 00:00:17|\n",
            "|0E13475422CC97210F6727A434EDCE93|2013-01-01 00:00:14|\n",
            "|0F4A6D743CD63B2BAB52B24332654F27|2013-01-01 00:00:24|\n",
            "|1285BE8E37D503B14657341C956CD9DF|2013-01-01 00:00:14|\n",
            "|13703B7576B6AF365ADCC05A7A93AB5E|2013-01-01 00:00:12|\n",
            "|1417D10B07A87BB9A9DBD32C93F70F87|2013-01-01 00:00:22|\n",
            "|14E4034C94080E9226C79826B55B0DF8|2013-01-01 00:00:19|\n",
            "|162A967C11C5A4839059F1B1C9868C33|2013-01-01 00:00:15|\n",
            "|16DBB69DD1C7E67AA1685FFE560D579E|2013-01-01 00:00:15|\n",
            "+--------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------------------+-------------------+\n",
            "|medallion                       |pickup_datetime    |\n",
            "+--------------------------------+-------------------+\n",
            "|003D87DB553C6F00F774C8575BC8444A|2013-01-01 00:00:16|\n",
            "|04C731091D60CE8356322C8AB24253B0|2013-01-01 00:00:24|\n",
            "|05656E0120B5B5A98016D9015A84A2C6|2013-01-01 00:00:24|\n",
            "|05CBBA266C228F2E5DB81C94F32A3E84|2013-01-01 00:00:26|\n",
            "|05E791EBACCC57E096D5847D34589DF1|2013-01-01 00:00:25|\n",
            "|06EFA41FDAD03EA54DA35A9B4BF55792|2013-01-01 00:00:17|\n",
            "|06F7086BC8CC3CB629115748433379D2|2013-01-01 00:00:27|\n",
            "|098FF67267798F397F82E0922FA70FAB|2013-01-01 00:00:14|\n",
            "|0A7E2DB544D098553DDEF21DE361DD95|2013-01-01 00:00:25|\n",
            "|0C3E065904E923C33696B809E1FE1538|2013-01-01 00:00:19|\n",
            "|0CAC8E999E3D3D98C663789471618315|2013-01-01 00:00:23|\n",
            "|0D0DD62C39826CDB4BAD198BC3968811|2013-01-01 00:00:23|\n",
            "|0D935422D6BCC25DC47195A5984BB9C2|2013-01-01 00:00:19|\n",
            "|0DE106C22F91831CF67D04EAE7973F02|2013-01-01 00:00:25|\n",
            "|0E118EB8E7FD2ED2A8EBDC64F96980FD|2013-01-01 00:00:24|\n",
            "|11815B094F2F5B2399C3FB55263E5E00|2013-01-01 00:00:26|\n",
            "|13C1BCFC3B73B6ADBA4212B1CC7AA615|2013-01-01 00:00:19|\n",
            "|15AFE3548AF724114C3644AA43E3A068|2013-01-01 00:00:21|\n",
            "|160801778A042001E99C5C008F02A324|2013-01-01 00:00:10|\n",
            "|1E4422E1D46FF8824C0B9138B8657058|2013-01-01 00:00:12|\n",
            "+--------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------------------+-------------------+\n",
            "|medallion                       |pickup_datetime    |\n",
            "+--------------------------------+-------------------+\n",
            "|CC2FB4FEA7B2817988542CC5607EA6E2|2013-01-01 00:00:18|\n",
            "|94B75E5069F7903E55F9883B42A24EB9|2013-01-01 00:00:15|\n",
            "|00B7EA9C3A09BEFD804B6C5465B92D56|2013-01-01 00:00:09|\n",
            "|012F172C0351A4767F60F6B6B2CF9E86|2013-01-01 00:00:29|\n",
            "|01D13A056D9A26F84C328DFDD5534B55|2013-01-01 00:00:22|\n",
            "|02A2214E583CDF23AE05E30966BFEFEB|2013-01-01 00:00:23|\n",
            "|02CDF4B814C18DE3898FDA761EA8A5B4|2013-01-01 00:00:28|\n",
            "|03D949650C9CF6A7A797C37022D14A62|2013-01-01 00:00:10|\n",
            "|057CFB402254395B4EE4CC11F10474CF|2013-01-01 00:00:28|\n",
            "|059E5BF1D421092E005D3A74304CEED3|2013-01-01 00:00:19|\n",
            "|066E412905F56CE0CF60D9393627A978|2013-01-01 00:00:24|\n",
            "|06A38297515E1320AE12B85258FA27B6|2013-01-01 00:00:20|\n",
            "|0781677ADD121C0A9EAA66F5E17F88B3|2013-01-01 00:00:28|\n",
            "|07D9B361B73F6880460BB19E4FC76E32|2013-01-01 00:00:30|\n",
            "|085667DDB5779227D5FA7B6C623EE535|2013-01-01 00:00:21|\n",
            "|0CD573F3014899C3C62C647A8CE4CF19|2013-01-01 00:00:30|\n",
            "|0CFA82B6C8EBE798637305981D3C557E|2013-01-01 00:00:03|\n",
            "|0E84DA4EE45E80A99CC65DC8D456E997|2013-01-01 00:00:25|\n",
            "|0F2293E74CC9DE6E6986069B7DD4FC8C|2013-01-01 00:00:28|\n",
            "|10CC1511C786DCD23292B2A505DFB854|2013-01-01 00:00:14|\n",
            "+--------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------------------+-------------------+\n",
            "|medallion                       |pickup_datetime    |\n",
            "+--------------------------------+-------------------+\n",
            "|8D90E4C96A0C162BC9A663F7F3C45379|2013-01-01 00:00:23|\n",
            "|7EA00F02D7FD4A26F786B960796B53D2|2013-01-01 00:00:27|\n",
            "|AE9D6880AEEF221B26DB42824E739223|2013-01-01 00:00:25|\n",
            "|00D07524C1482FF5A3CB0932BE29003E|2013-01-01 00:00:18|\n",
            "|00EEA9525C3C7B6DF582BFBDBB938A7D|2013-01-01 00:00:21|\n",
            "|01389E9CF7758ECAC205EE26B2DD542A|2013-01-01 00:00:22|\n",
            "|037CB433AB5895649F7A9EF37F767EF1|2013-01-01 00:00:30|\n",
            "|03A6307BA4258D12260DFC2E74CEA7B4|2013-01-01 00:00:36|\n",
            "|03B390017C7E2B4FB9652DE6239C33FB|2013-01-01 00:00:31|\n",
            "|0531373C01FD1416769E34F5525B54C8|2013-01-01 00:00:30|\n",
            "|062281602740F289FB3C49AD21C5A973|2013-01-01 00:00:30|\n",
            "|067D49BE6232063BC77BE9619D0B08B6|2013-01-01 00:00:29|\n",
            "|06808F2B069EBFA72B16977B7C4672C4|2013-01-01 00:00:26|\n",
            "|07290D3599E7A0D62097A346EFCC1FB5|2013-01-01 00:00:36|\n",
            "|0794ECE3C5AD2BDEEE811C487D6CDC8B|2013-01-01 00:00:21|\n",
            "|0964BAF015F74B4E628615EAFD0E2D57|2013-01-01 00:00:31|\n",
            "|0AA19E9173F2672FC974C7B77412E0D7|2013-01-01 00:00:29|\n",
            "|0B593CED4C3F363DD4ED3F745B651CB1|2013-01-01 00:00:35|\n",
            "|0D935422D6BCC25DC47195A5984BB9C2|2013-01-01 00:00:30|\n",
            "|0DA6AF03BF924F21C22BE75217246DEA|2013-01-01 00:00:26|\n",
            "+--------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGXlP2BH6uox"
      },
      "source": [
        "### Spark Streaming\n",
        "\n",
        "The code below shows how to access the kafka stream in the original spark streaming mode. Note that with this setup, we need to connect to `localhost`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR1S1B7T62II"
      },
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "sc = SparkContext(\"local[*]\", \"Kafka Spark Streaming Example\")\n",
        "\n",
        "ssc = StreamingContext(sc, 1)\n",
        "lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
        "            {\"metadata.broker.list\": \"localhost:9092\"}) \\\n",
        "        .map( lambda e : e[1] ) \\\n",
        "        .filter( lambda line: len(line) > 0)\n",
        "\n",
        "\n",
        "lines.pprint()\n",
        "    \n",
        "ssc.start()\n",
        "ssc.awaitTermination(20)\n",
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}