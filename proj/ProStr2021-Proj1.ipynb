{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"colab":{"name":"ProStr2021-Proj.ipynb","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ProStr 2021 Project 1\nThe project scenario involves a dataset of taxi rides, collected circa 2013, in the New York city area.\n\nEach taxi ride corresponds to an event in the dataset, comprising of the passenger pick-up and drop-off points, and respective timestamps, as well as information related to the payment, the taxi and its driver.\n\nThis project scenario is inspired by the [ACM DEBS 2015 Grand Challenge](http://www.debs2015.org/call-grand-challenge.html).","metadata":{"id":"7SnHMsPsGTY6"}},{"cell_type":"markdown","source":"## Taxi Ride Event\n\nEach taxi ride event comprises a number of attributes, as follows:\n\n| Attribute   | Description |\n| :---        |        :--- |\n|medallion| an md5sum of the identifier of the taxi - vehicle bound|\n|hack_license| an md5sum of the identifier for the taxi license|\n|pickup_datetime| time when the passenger(s) were picked up|\n|dropoff_datetime| time when the passenger(s) were dropped off|\n|trip_time_in_secs| duration of the trip|\n|trip_distance| trip distance in miles|\n|pickup_longitude| longitude coordinate of the pickup location|\n|pickup_latitude| latitude coordinate of the pickup location|\n|dropoff_longitude| longitude coordinate of the drop-off location|\n|dropoff_latitude| latitude coordinate of the drop-off location|\n|payment_type| the payment method - credit card or cash|\n|fare_amount| fare amount in dollars|\n|surcharge| surcharge in dollars|\n|mta_tax| tax in dollars|\n|tip_amount| tip in dollars|\n|tolls_amount| bridge and tunnel tolls in dollars|\n|total_amount| total paid amount in dollars|\n\nEach event is published as a text string, with the attributes separated by commas.\n","metadata":{"id":"VPV15xTkGTY9"}},{"cell_type":"markdown","source":"## Dataset\n\nThe dataset is available in two forms:\n\n* Sample of 20 days (roughly 2 million events) of data (~ 130 MB) [download](https://drive.google.com/file/d/0B0TBL8JNn3JgTGNJTEJaQmFMbk0/view?usp=sharing)\n* The whole year of 2013 (~ 173 million events) (~ 12 GB) (~33 GB expanded) [download](https://drive.google.com/file/d/0B4zFfvIVhcMzcWV5SEQtSUdtMWc/view?usp=sharing)\n\n---\n\n* Events are reported at the end of the trip, i.e., upon arrival in the order of the drop-off timestamps.\n\n* Events with the same *dropoff_datetime* are in random order.\n\n* Quality of the data is **not perfect**.\n\n + Some events might miss information such as *drop off* and *pickup*;\n\n + Moreover, some information, such as, e.g., the *fare price*, might have been entered incorrectly by the taxi drivers thus introducing additional skew.","metadata":{"id":"4RctLDtdGTY-"}},{"cell_type":"markdown","source":"## Requeriments\n\nOut of the following 4 queries, you need to solve **a minimum of 2** (two).\n\nQueries are marked with a number of **€**, as an indication of their expected dificulty and grading points.\n\n---\n\n#### Delivery Format\n\nYou can use either Spark Streaming or Spart Structured Streaming.\n\nThe solution should be delivered as a jupyter notebook. \n\n---\n\n#### Bonus\n\n* Solve each query using a different framework (e.g, Q1: Spark Streaming, Q2: Spark SQL)\n\n* Solve a third query.\n\n---\n\n#### Grading\n\nGrading will take into consideration the overall presentation quality of jupyter notebook, the correctness of the solution, the quality of the code, and the summary pdf report, where the results are discussed.\n\n---\n\n#### DEADLINE\n\n9th May 2021, 23h59","metadata":{}},{"cell_type":"markdown","source":"## Queries","metadata":{}},{"cell_type":"markdown","source":"### Q1: Find the top 10 most frequent routes during the last 30 minutes. (€)\n\n• A route is represented by a starting grid cell and an ending grid cell;\n\n• All routes completed within the last 30 minutes are considered for the query;\n\n• Use a grid of 300x300 cells, corresponding to square of 500x500m. See HelperCode.\n\n• All trips starting or ending outside this area are treated as outliers (not be considered)\n\n---\n\n• Ideally, the output query results should be updated whenever any of the 10 most frequent routes changes;","metadata":{}},{"cell_type":"markdown","source":"### Q2: Identify areas that are currently most profitable for taxi drivers. (€€€)\n\nThe profitability of an area is determined by dividing the area profit by the number of empty taxis in that area within the last 15 minutes.\n    \nThe profit that originates from an area is computed by calculating the average fare + tip for trips that started in the area and ended within the last 15 minutes.\n\nThe number of empty taxis in an area is the sum of taxis that had a drop-off location in that area less than 30 minutes ago and had no following pickup yet.\n\nNote: Unlike in the original DEBS Challenge, use the same 300x300 grid, as in Q1.","metadata":{}},{"cell_type":"markdown","source":"### Q3: Detect \"Anomalous\" Rides (€€)\n\nProvide an answer to the following question: \"Are all rides fair?\"\n\nDetect rides that cost more or take longer than expected. \n\nTo compute the expected duration or expected cost of ride, use average values computed over the last 1 hour;","metadata":{}},{"cell_type":"markdown","source":"### Q4: Detect \"Anomalous\" Drivers (€€)\n\nProvide an answer to the following question: \"Are all drivers equal?\"\n\nDetect drivers that seem to deviate from the pack in some way.\n\nThe criteria used to diferentiate drivers is up to you. As a suggestion, are there drivers that\nare more efficient, i.e., earn more and drive less time or distance?","metadata":{}},{"cell_type":"markdown","source":"## Suggestions\n\n* Read all the available information in [Debs Challenge](http://www.debs2015.org/call-grand-challenge.html);\n\n* Get familiar with the sample data;\n\n* Sanitize the data: i.e, exclude incomplete, non used data or out of area rides;\n\n* Compute Streams with converted coordinates to cell grids.\n\n    Simplified flat earth assumption for mapping coordinates to cells in the queries. \n    Moving 500 meters south corresponds to a change of 0.004491556 degrees in latitude. Moving 500 meters east,\n    corresponds to 0.005986 degrees in longitude. \n\n    Use or adapt the supplied code below.\n    \n* During development define shorter windows to aggregate and preview results faster.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Addendum\n\n### Python code to get some stats from the sample dataset...\nUpload `sample.csv.gz` before running...","metadata":{}},{"cell_type":"code","source":"import gzip\nimport csv\n\nevents = 0\nstats = {'#Taxis' : set(), '#Drivers' : set() }\nwith gzip.open('sample.csv.gz','rt') as f:\n    for line in f:\n        tokens = line.split(' ')\n        medallion = tokens[0]\n        driver = tokens[1]\n        stats['#Taxis'].add(medallion)\n        stats['#Drivers'].add(driver)\n        events = events + 1\n\nprint('#Events: {}'.format(events))\nfor k,v in stats.items():\n    print(k, ': ', len(v))\n","metadata":{"id":"Dh7Ys0CVGTY-","outputId":"d30bd6a1-7b1e-45c3-e9bf-42cf637d7d9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### Kafka Streams\n\nTo fully leverage Spark Streaming, the taxi ride dataset can be accessed\nas an [Apache Kafka](https://kafka.apache.org/) stream. \n\nApache Kafka is topic-based publish/subscribe broker platform,\noffering a reliable and persistent event dissemination service.\n\nEach taxi ride is published as a discrete event, under the `debs`\ntopic.\n\n*Spark Streaming* and *Spark Structured Streaming* can ingest Kafka event sources, as explained next.","metadata":{"id":"JUP-5m_KGTY_"}},{"cell_type":"markdown","source":"### Setup\n\n1. Start kafka\n\n * Download and execute [`./start-kakfa.sh`](https://github.com/smduarte/ps2021/blob/master/proj/setup/start-kafka.sh)\n    \n    \n2. Start the Debs Taxi Ride publisher.\n\n * Download and execute:\n    [`./publish-debs.sh`](https://github.com/smduarte/ps2021/blob/master/proj/setup/publish-debs.sh)\n \n \nIn Linux/macOs, you may need make the scripts executable:\n \n * `chmod a+x <script>`","metadata":{}},{"cell_type":"markdown","source":"### Ingesting Events\n\n---\n\nNote: The Kafka source for *Spark Streaming* is only available for **pyspark** up to\nversion 2.4.x of Spark. \n\nFor newer versions of Spark we are limited to *Spark SQL Structured Streaming*.\n\n\nWe will use Spark version 2.4.5 as it works with both streaming frameworks.","metadata":{}},{"cell_type":"code","source":"!spark-submit --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SparkStreaming\n\nThe example below shows how to create a *DStream* from\na kafka topic. \n\nBoth *Kafka* and *Debs Publisher* need to be running already.\n\nIn the example, `kafka:9092` is the name of the machine/container\nand port where the Apache Kafka broker is running.\n\n`debs` is the event topic the *publisher* uses.","metadata":{}},{"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.streaming.kafka import KafkaUtils\n\nsc = SparkContext(\"local[*]\", \"Kafka Spark Streaming Example\")\n\nssc = StreamingContext(sc, 1)\nlines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n            {\"metadata.broker.list\": \"kafka:9092\"}) \\\n        .map( lambda e : e[1] ) \\\n        .filter( lambda line: len(line) > 0)\n\n\nlines.pprint()\n    \nssc.start()\nssc.awaitTermination(20)\nssc.stop()\nsc.stop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Spark SQL (Structured Streaming)\n\nThe example below shows how to prepare Spark for\ningesting and processing Kafka events using\nthe structured API. \n\nComplete by adding the columns you need for the assigment.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Kafka Spark Structured Streaming Example\") \\\n    .getOrCreate()\n\nlines = spark \\\n  .readStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n  .option(\"subscribe\", \"debs\") \\\n  .load() \\\n  .selectExpr(\"CAST(value AS STRING)\")\n\nsplit_lines = split(lines['value'], ',')\n\nrides = lines.withColumn('medallion', split_lines.getItem(0).cast(\"string\")) \\\n        .withColumn('pickup_datetime', split_lines.getItem(2).cast(\"timestamp\")) \\\n        .drop('value')\n\nquery = rides \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .trigger(processingTime='5 seconds') \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination( 20)\nquery.stop()\nspark.stop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Helper Code\n\nThe following helper functions can be used in the assignment, \nas is or changed as needed.\n","metadata":{}},{"cell_type":"markdown","source":"#### Convert GPS coordinates to grid cell coordinates","metadata":{}},{"cell_type":"code","source":"# Longitude and latitude from the upper left corner of the grid\nMIN_LON = -74.916578\nMAX_LAT = 41.47718278\n\n# Longitude and latitude that correspond to a shift in 500 meters\nLON_DELTA = 0.005986\nLAT_DELTA = 0.004491556\n\ndef latlon_to_grid(lat, lon):\n    return ((int)((MAX_LAT - lat)/LAT_DELTA), (int)((lon - MIN_LON)/LON_DELTA))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In Bounds check\n\nYou can use cell coordinates to exclude invalid rides ","metadata":{}},{"cell_type":"code","source":"def inBounds( cell ):\n    return cell[0] > 0 and cell[0] < 300 and cell[1] > 0 and cell[1] < 300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Parsing timestamps","metadata":{}},{"cell_type":"code","source":"import datetime\n\ndef parseTime( date_time_str ):\n    return datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}