{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "ProStr2021-Proj1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SnHMsPsGTY6"
      },
      "source": [
        "# ProStr 2021 Project 1\n",
        "The project scenario involves a dataset of taxi rides, collected circa 2013, in the New York city area.\n",
        "\n",
        "Each taxi ride corresponds to an event in the dataset, comprising of the passenger pick-up and drop-off points, and respective timestamps, as well as information related to the payment, the taxi and its driver.\n",
        "\n",
        "This project scenario is inspired by the [ACM DEBS 2015 Grand Challenge](http://www.debs2015.org/call-grand-challenge.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPV15xTkGTY9"
      },
      "source": [
        "## Taxi Ride Event\n",
        "\n",
        "Each taxi ride event comprises a number of attributes, as follows:\n",
        "\n",
        "| Attribute   | Description |\n",
        "| :---        |        :--- |\n",
        "|medallion| an md5sum of the identifier of the taxi - vehicle bound|\n",
        "|hack_license| an md5sum of the identifier for the taxi license|\n",
        "|pickup_datetime| time when the passenger(s) were picked up|\n",
        "|dropoff_datetime| time when the passenger(s) were dropped off|\n",
        "|trip_time_in_secs| duration of the trip|\n",
        "|trip_distance| trip distance in miles|\n",
        "|pickup_longitude| longitude coordinate of the pickup location|\n",
        "|pickup_latitude| latitude coordinate of the pickup location|\n",
        "|dropoff_longitude| longitude coordinate of the drop-off location|\n",
        "|dropoff_latitude| latitude coordinate of the drop-off location|\n",
        "|payment_type| the payment method - credit card or cash|\n",
        "|fare_amount| fare amount in dollars|\n",
        "|surcharge| surcharge in dollars|\n",
        "|mta_tax| tax in dollars|\n",
        "|tip_amount| tip in dollars|\n",
        "|tolls_amount| bridge and tunnel tolls in dollars|\n",
        "|total_amount| total paid amount in dollars|\n",
        "\n",
        "Each event is published as a text string, with the attributes separated by commas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RctLDtdGTY-"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset is available in two forms:\n",
        "\n",
        "* Sample of 20 days (roughly 2 million events) of data (~ 130 MB) [download](https://drive.google.com/file/d/1jF_YmKFskdNgchtUb0GGaUyfxAjMUHdw/view?usp=sharing)\n",
        "* The whole year of 2013 (~ 173 million events) (~ 12 GB) (~33 GB expanded) [download](https://drive.google.com/file/d/0B4zFfvIVhcMzcWV5SEQtSUdtMWc/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "* Events are reported at the end of the trip, i.e., upon arrival in the order of the drop-off timestamps.\n",
        "\n",
        "* Events with the same *dropoff_datetime* are in random order.\n",
        "\n",
        "* Quality of the data is **not perfect**.\n",
        "\n",
        " + Some events might miss information such as *drop off* and *pickup*;\n",
        "\n",
        " + Moreover, some information, such as, e.g., the *fare price*, might have been entered incorrectly by the taxi drivers thus introducing additional skew."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXBQBe_JVfpL"
      },
      "source": [
        "## Requeriments\n",
        "\n",
        "Out of the following 4 queries, you need to solve **a minimum of 2** (two).\n",
        "\n",
        "Queries are marked with a number of **€**, as an indication of their expected dificulty and grading points.\n",
        "\n",
        "---\n",
        "\n",
        "#### Delivery Format\n",
        "\n",
        "You can use either Spark Streaming or Spart Structured Streaming.\n",
        "\n",
        "The solution should be delivered as a jupyter notebook. \n",
        "\n",
        "---\n",
        "\n",
        "#### Bonus\n",
        "\n",
        "* Solve each query using a different framework (e.g, Q1: Spark Streaming, Q2: Spark SQL)\n",
        "\n",
        "* Solve a third query.\n",
        "\n",
        "---\n",
        "\n",
        "#### Grading\n",
        "\n",
        "Grading will take into consideration the overall presentation quality of jupyter notebook, the correctness of the solution, the quality of the code, and the summary pdf report, where the results are discussed.\n",
        "\n",
        "---\n",
        "\n",
        "#### DEADLINE\n",
        "\n",
        "9th May 2021, 23h59"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGIaAo_CVfpM"
      },
      "source": [
        "## Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiGwDRCCVfpM"
      },
      "source": [
        "### Q1: Find the top 10 most frequent routes during the last 30 minutes. (€)\n",
        "\n",
        "• A route is represented by a starting grid cell and an ending grid cell;\n",
        "\n",
        "• All routes completed within the last 30 minutes are considered for the query;\n",
        "\n",
        "• Use a grid of 300x300 cells, corresponding to square of 500x500m. See HelperCode.\n",
        "\n",
        "• All trips starting or ending outside this area are treated as outliers (not be considered)\n",
        "\n",
        "---\n",
        "\n",
        "• Ideally, the output query results should be updated whenever any of the 10 most frequent routes changes;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhvJVB7oVfpM"
      },
      "source": [
        "### Q2: Identify areas that are currently most profitable for taxi drivers. (€€€)\n",
        "\n",
        "The profitability of an area is determined by dividing the area profit by the number of empty taxis in that area within the last 15 minutes.\n",
        "    \n",
        "The profit that originates from an area is computed by calculating the average fare + tip for trips that started in the area and ended within the last 15 minutes.\n",
        "\n",
        "The number of empty taxis in an area is the sum of taxis that had a drop-off location in that area less than 30 minutes ago and had no following pickup yet.\n",
        "\n",
        "Note: Unlike in the original DEBS Challenge, use the same 300x300 grid, as in Q1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lJkTptvVfpN"
      },
      "source": [
        "### Q3: Detect \"Anomalous\" Rides (€€)\n",
        "\n",
        "Provide an answer to the following question: \"Are all rides fair?\"\n",
        "\n",
        "Detect rides that cost more or take longer than expected. \n",
        "\n",
        "To compute the expected duration or expected cost of ride, use average values computed over the last 1 hour;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMUtS9jYVfpN"
      },
      "source": [
        "### Q4: Detect \"Anomalous\" Drivers (€€)\n",
        "\n",
        "Provide an answer to the following question: \"Are all drivers equal?\"\n",
        "\n",
        "Detect drivers that seem to deviate from the pack in some way.\n",
        "\n",
        "The criteria used to diferentiate drivers is up to you. As a suggestion, are there drivers that\n",
        "are more efficient, i.e., earn more and drive less time or distance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld9HkdN2VfpO"
      },
      "source": [
        "## Suggestions\n",
        "\n",
        "* Read all the available information in [Debs Challenge](http://www.debs2015.org/call-grand-challenge.html);\n",
        "\n",
        "* Get familiar with the sample data;\n",
        "\n",
        "* Sanitize the data: i.e, exclude incomplete, non used data or out of area rides;\n",
        "\n",
        "* Compute Streams with converted coordinates to cell grids.\n",
        "\n",
        "    Simplified flat earth assumption for mapping coordinates to cells in the queries. \n",
        "    Moving 500 meters south corresponds to a change of 0.004491556 degrees in latitude. Moving 500 meters east,\n",
        "    corresponds to 0.005986 degrees in longitude. \n",
        "\n",
        "    Use or adapt the supplied code below.\n",
        "    \n",
        "* During development define shorter windows to aggregate and preview results faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxbuspK2VfpO"
      },
      "source": [
        "---\n",
        "\n",
        "## Addendum\n",
        "\n",
        "### Python code to get some stats from the sample dataset...\n",
        "Upload `sample.csv.gz` before running..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh7Ys0CVGTY-",
        "trusted": true
      },
      "source": [
        "import gzip\n",
        "import csv\n",
        "\n",
        "events = 0\n",
        "stats = {'#Taxis' : set(), '#Drivers' : set() }\n",
        "with gzip.open('sample.csv.gz','rt') as f:\n",
        "    for line in f:\n",
        "        tokens = line.split(' ')\n",
        "        medallion = tokens[0]\n",
        "        driver = tokens[1]\n",
        "        stats['#Taxis'].add(medallion)\n",
        "        stats['#Drivers'].add(driver)\n",
        "        events = events + 1\n",
        "\n",
        "print('#Events: {}'.format(events))\n",
        "for k,v in stats.items():\n",
        "    print(k, ': ', len(v))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB6xlFhjVfpP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUP-5m_KGTY_"
      },
      "source": [
        "### Kafka Streams\n",
        "\n",
        "To fully leverage Spark Streaming, the taxi ride dataset can be accessed\n",
        "as an [Apache Kafka](https://kafka.apache.org/) stream. \n",
        "\n",
        "Apache Kafka is topic-based publish/subscribe broker platform,\n",
        "offering a reliable and persistent event dissemination service.\n",
        "\n",
        "Each taxi ride is published as a discrete event, under the `debs`\n",
        "topic.\n",
        "\n",
        "*Spark Streaming* and *Spark Structured Streaming* can ingest Kafka event sources, as explained next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L-rM1mFVfpQ"
      },
      "source": [
        "### Setup\n",
        "\n",
        "1. Start kafka\n",
        "\n",
        " * Download and execute [`./start-kakfa.sh`](https://github.com/smduarte/ps2021/blob/master/proj/setup/start-kafka.sh)\n",
        "    \n",
        "    \n",
        "2. Start the Debs Taxi Ride publisher.\n",
        "\n",
        " * Download and execute:\n",
        "    [`./publish-debs.sh`](https://github.com/smduarte/ps2021/blob/master/proj/setup/publish-debs.sh)\n",
        " \n",
        " \n",
        "In Linux/macOs, you may need make the scripts executable:\n",
        " \n",
        " * `chmod a+x <script>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_oLZM1VfpQ"
      },
      "source": [
        "### Ingesting Events\n",
        "\n",
        "---\n",
        "\n",
        "Note: The Kafka source for *Spark Streaming* is only available for **pyspark** up to\n",
        "version 2.4.x of Spark. \n",
        "\n",
        "For newer versions of Spark we are limited to *Spark SQL Structured Streaming*.\n",
        "\n",
        "\n",
        "We will use Spark version 2.4.5 as it works with both streaming frameworks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MfvCZzdLVfpQ"
      },
      "source": [
        "!spark-submit --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYIFHOyhVfpR"
      },
      "source": [
        "#### SparkStreaming\n",
        "\n",
        "The example below shows how to create a *DStream* from\n",
        "a kafka topic. \n",
        "\n",
        "Both *Kafka* and *Debs Publisher* need to be running already.\n",
        "\n",
        "In the example, `kafka:9092` is the name of the machine/container\n",
        "and port where the Apache Kafka broker is running.\n",
        "\n",
        "`debs` is the event topic the *publisher* uses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JQu9LNeBVfpR"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "\n",
        "sc = SparkContext(\"local[*]\", \"Kafka Spark Streaming Example\")\n",
        "\n",
        "ssc = StreamingContext(sc, 1)\n",
        "lines = KafkaUtils.createDirectStream(ssc, [\"debs\"], \\\n",
        "            {\"metadata.broker.list\": \"kafka:9092\"}) \\\n",
        "        .map( lambda e : e[1] ) \\\n",
        "        .filter( lambda line: len(line) > 0)\n",
        "\n",
        "\n",
        "lines.pprint()\n",
        "    \n",
        "ssc.start()\n",
        "ssc.awaitTermination(20)\n",
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ZkG2IsVfpR"
      },
      "source": [
        "#### Spark SQL (Structured Streaming)\n",
        "\n",
        "The example below shows how to prepare Spark for\n",
        "ingesting and processing Kafka events using\n",
        "the structured API. \n",
        "\n",
        "Complete by adding the columns you need for the assigment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OHBfNFSNVfpR"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(20, False)\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Kafka Spark Structured Streaming Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
        "  .option(\"subscribe\", \"debs\") \\\n",
        "  .load() \\\n",
        "  .selectExpr(\"CAST(value AS STRING)\")\n",
        "\n",
        "split_lines = split(lines['value'], ',')\n",
        "\n",
        "rides = lines.withColumn('medallion', split_lines.getItem(0).cast(\"string\")) \\\n",
        "        .withColumn('pickup_datetime', split_lines.getItem(2).cast(\"timestamp\")) \\\n",
        "        .drop('value')\n",
        "\n",
        "query = rides \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination( 20)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8odCTKXVfpS"
      },
      "source": [
        "### Helper Code\n",
        "\n",
        "The following helper functions can be used in the assignment, \n",
        "as is or changed as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1sWaAD1VfpS"
      },
      "source": [
        "#### Convert GPS coordinates to grid cell coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qReYRLQzVfpS"
      },
      "source": [
        "# Longitude and latitude from the upper left corner of the grid\n",
        "MIN_LON = -74.916578\n",
        "MAX_LAT = 41.47718278\n",
        "\n",
        "# Longitude and latitude that correspond to a shift in 500 meters\n",
        "LON_DELTA = 0.005986\n",
        "LAT_DELTA = 0.004491556\n",
        "\n",
        "def latlon_to_grid(lat, lon):\n",
        "    return ((int)((MAX_LAT - lat)/LAT_DELTA), (int)((lon - MIN_LON)/LON_DELTA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN-CicaoVfpT"
      },
      "source": [
        "#### In Bounds check\n",
        "\n",
        "You can use cell coordinates to exclude invalid rides "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wQ8ObEC1VfpT"
      },
      "source": [
        "def inBounds( cell ):\n",
        "    return cell[0] > 0 and cell[0] < 300 and cell[1] > 0 and cell[1] < 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIiZnpl_VfpT"
      },
      "source": [
        "#### Parsing timestamps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RU4CkVB3VfpT"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def parseTime( date_time_str ):\n",
        "    return datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}