{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spark streaming Lecture 3 examples\n\nThese are the examples used in the slides of lecture 3.\n\nThe code assumes that the following streams are available:\n* web.log at server logsender, port 7777. \n\n`docker run --name logsender --network psnet -d smduarte/ps2021-logsender`\n\n* simple.log at server logsender, port 7776. This is a simple stream with an event every 5 second, with the exception of a few periods where up to 3 events were produced.\n\n`docker exec -d logsender python ./server.py /data/simple.log 7776`","metadata":{}},{"cell_type":"markdown","source":"## Example 1\n\nList the data frames produced while processing the stream.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n  \nspark = SparkSession \\\n    .builder \\\n    .appName(\"StructuredWebLogExample\") \\\n    .getOrCreate()\n\n# Create DataFrame representing the stream of input \n# lines from connection to logsender 7776\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"logsender\") \\\n    .option(\"port\", 7776) \\\n    .load() \n\n# Just dump the created data frames\nquery = lines \\\n    .writeStream \\\n    .outputMode(\"append\") \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination(20)\nquery.stop()\n","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"+-----+\n|value|\n+-----+\n+-----+\n\n+--------------------------------------------------------------------+\n|value                                                               |\n+--------------------------------------------------------------------+\n|2020-03-15T10:00:00.000+0000 37.139.9.11 200 GET /date/10h00m00s 0.1|\n+--------------------------------------------------------------------+\n\n+---------------------------------------------------------------------+\n|value                                                                |\n+---------------------------------------------------------------------+\n|2020-03-15T10:00:05.000+0000 37.139.9.12 200 GET /date/10h00m05s 0.12|\n+---------------------------------------------------------------------+\n\n+--------------------------------------------------------------------+\n|value                                                               |\n+--------------------------------------------------------------------+\n|2020-03-15T10:00:10.000+0000 37.139.9.13 200 GET /date/10h00m10s 0.2|\n+--------------------------------------------------------------------+\n\n+--------------------------------------------------------------------+\n|value                                                               |\n+--------------------------------------------------------------------+\n|2020-03-15T10:00:15.000+0000 37.139.9.14 200 GET /date/10h00m15s 0.1|\n+--------------------------------------------------------------------+\n\n+--------------------------------------------------------------------+\n|value                                                               |\n+--------------------------------------------------------------------+\n|2020-03-15T10:00:17.000+0000 37.139.9.24 200 GET /date/10h00m17s 0.1|\n+--------------------------------------------------------------------+\n\n+--------------------------------------------------------------------+\n|value                                                               |\n+--------------------------------------------------------------------+\n|2020-03-15T10:00:19.000+0000 37.139.9.34 200 GET /date/10h00m19s 0.1|\n+--------------------------------------------------------------------+\n\n+--------------------------------------------------------------------+\n|value                                                               |\n+--------------------------------------------------------------------+\n|2020-03-15T10:00:20.000+0000 37.139.9.15 200 GET /date/10h00m20s 0.3|\n+--------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Example 2\n\nList the top-3 IP sources with more accesses.\n","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n  \nspark = SparkSession \\\n    .builder \\\n    .appName(\"StructuredWebLogExample\") \\\n    .getOrCreate()\n\n# Create DataFrame representing the stream of input \n# lines from connection to logsender 7776\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"logsender\") \\\n    .option(\"port\", 7777) \\\n    .load() \n\nsplit_lines = split(lines['value'], ' ')\n\nlines = lines.withColumn('time', split_lines.getItem(0).cast(\"timestamp\")) \\\n    .withColumn('IP', split_lines.getItem(1).cast(\"string\")) \\\n    .withColumn('code', split_lines.getItem(2).cast(\"integer\")) \\\n    .withColumn('op', split_lines.getItem(3).cast(\"string\")) \\\n    .withColumn('URL', split_lines.getItem(4).cast(\"string\")) \\\n    .withColumn('dur', split_lines.getItem(5).cast(\"float\")) \\\n    .drop('value')\n\nquery = lines.groupBy('IP') \\\n    .count() \\\n    .orderBy('count', ascending=False) \\\n    .limit(3)\n\n# Dump the results\nquery = query \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination(20)\nquery.stop()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same program with alternative computations.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import count\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n  \nspark = SparkSession \\\n    .builder \\\n    .appName(\"StructuredWebLogExample\") \\\n    .getOrCreate()\n\n# Create DataFrame representing the stream of input \n# lines from connection to logsender 7776\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"logsender\") \\\n    .option(\"port\", 7777) \\\n    .load() \n\nsplit_lines = split(lines['value'], ' ')\n\nlines = lines.withColumn('time', split_lines.getItem(0).cast(\"timestamp\")) \\\n    .withColumn('IP', split_lines.getItem(1).cast(\"string\")) \\\n    .withColumn('code', split_lines.getItem(2).cast(\"integer\")) \\\n    .withColumn('op', split_lines.getItem(3).cast(\"string\")) \\\n    .withColumn('URL', split_lines.getItem(4).cast(\"string\")) \\\n    .withColumn('dur', split_lines.getItem(5).cast(\"float\")) \\\n    .drop('value')\n\nquery = lines.groupBy('IP') \\\n    .agg(count('*').alias('count')) \\\n    .orderBy('count',ascending=False) \\\n    .limit(3)\n\n# Dump the results\nquery = query \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination(20)\nquery.stop()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example 3\n\nExample 2 using SQL.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n  \nspark = SparkSession \\\n    .builder \\\n    .appName(\"StructuredWebLogExample\") \\\n    .getOrCreate()\n\n# Create DataFrame representing the stream of input \n# lines from connection to logsender 7776\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"logsender\") \\\n    .option(\"port\", 7777) \\\n    .load() \n\nsplit_lines = split(lines['value'], ' ')\nlines = lines.withColumn('time', split_lines.getItem(0).cast(\"timestamp\")) \\\n    .withColumn('IP', split_lines.getItem(1).cast(\"string\")) \\\n    .withColumn('code', split_lines.getItem(2).cast(\"integer\")) \\\n    .withColumn('op', split_lines.getItem(3).cast(\"string\")) \\\n    .withColumn('URL', split_lines.getItem(4).cast(\"string\")) \\\n    .withColumn('dur', split_lines.getItem(5).cast(\"float\")) \\\n    .drop('value')\n\nlines.createOrReplaceTempView(\"weblog\")\n\nquery = spark.sql(\"SELECT IP, COUNT(*) AS count FROM weblog GROUP BY IP ORDER BY count DESC LIMIT 3\")\n\n# Dump the results\nquery = query \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination(20)\nquery.stop()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example 4\n\nList the top-3 IP sources with more accesses in the last 30 seconds. Update the list every 10 seconds.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n  \nspark = SparkSession \\\n    .builder \\\n    .appName(\"StructuredWebLogExample\") \\\n    .getOrCreate()\n\n# Create DataFrame representing the stream of input \n# lines from connection to logsender 7776\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"logsender\") \\\n    .option(\"port\", 7777) \\\n    .load() \n\nsplit_lines = split(lines['value'], ' ')\nlines = lines.withColumn('time', split_lines.getItem(0).cast(\"timestamp\")) \\\n    .withColumn('IP', split_lines.getItem(1).cast(\"string\")) \\\n    .withColumn('code', split_lines.getItem(2).cast(\"integer\")) \\\n    .withColumn('op', split_lines.getItem(3).cast(\"string\")) \\\n    .withColumn('URL', split_lines.getItem(4).cast(\"string\")) \\\n    .withColumn('dur', split_lines.getItem(5).cast(\"float\")) \\\n    .drop('value')\n\nquery = lines.groupBy(window(lines.time, \"30 seconds\", \"10 seconds\"), 'IP') \\\n    .agg(count('*').alias('count')) \\\n    .orderBy('window', 'count', ascending=False) \\\n    .limit(3)\n\n# Just dump the created data frames\nquery = query \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination(60)\nquery.stop()\n","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"+------+---+-----+\n|window|IP |count|\n+------+---+-----+\n+------+---+-----+\n\n+------------------------------------------+--------------+-----+\n|window                                    |IP            |count|\n+------------------------------------------+--------------+-----+\n|[2016-12-06 08:58:40, 2016-12-06 08:59:10]|120.52.73.97  |152  |\n|[2016-12-06 08:58:40, 2016-12-06 08:59:10]|178.22.148.122|120  |\n|[2016-12-06 08:58:40, 2016-12-06 08:59:10]|120.52.73.98  |104  |\n+------------------------------------------+--------------+-----+\n\n+------------------------------------------+--------------+-----+\n|window                                    |IP            |count|\n+------------------------------------------+--------------+-----+\n|[2016-12-06 08:58:40, 2016-12-06 08:59:10]|120.52.73.97  |588  |\n|[2016-12-06 08:58:40, 2016-12-06 08:59:10]|120.52.73.98  |421  |\n|[2016-12-06 08:58:40, 2016-12-06 08:59:10]|178.22.148.122|348  |\n+------------------------------------------+--------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:58:50, 2016-12-06 08:59:20]|185.28.193.95|667  |\n|[2016-12-06 08:58:50, 2016-12-06 08:59:20]|120.52.73.97 |462  |\n|[2016-12-06 08:58:50, 2016-12-06 08:59:20]|120.52.73.98 |306  |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:58:50, 2016-12-06 08:59:20]|185.28.193.95|1328 |\n|[2016-12-06 08:58:50, 2016-12-06 08:59:20]|120.52.73.97 |1049 |\n|[2016-12-06 08:58:50, 2016-12-06 08:59:20]|120.52.73.98 |702  |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:59:00, 2016-12-06 08:59:30]|185.28.193.95|487  |\n|[2016-12-06 08:59:00, 2016-12-06 08:59:30]|120.52.73.97 |433  |\n|[2016-12-06 08:59:00, 2016-12-06 08:59:30]|120.52.73.98 |311  |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:59:00, 2016-12-06 08:59:30]|185.28.193.95|1611 |\n|[2016-12-06 08:59:00, 2016-12-06 08:59:30]|120.52.73.97 |1131 |\n|[2016-12-06 08:59:00, 2016-12-06 08:59:30]|120.52.73.98 |842  |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|185.28.193.95|286  |\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|120.52.73.97 |198  |\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|120.52.73.98 |144  |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|185.28.193.95|1197 |\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|120.52.73.97 |860  |\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|120.52.73.98 |664  |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|120.52.73.97 |1518 |\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|185.28.193.95|1207 |\n|[2016-12-06 08:59:10, 2016-12-06 08:59:40]|120.52.73.98 |1170 |\n+------------------------------------------+-------------+-----+\n\n+------------------------------------------+------------+-----+\n|window                                    |IP          |count|\n+------------------------------------------+------------+-----+\n|[2016-12-06 08:59:20, 2016-12-06 08:59:50]|120.52.73.97|389  |\n|[2016-12-06 08:59:20, 2016-12-06 08:59:50]|120.52.73.98|296  |\n|[2016-12-06 08:59:20, 2016-12-06 08:59:50]|97.77.104.22|200  |\n+------------------------------------------+------------+-----+\n\n+------------------------------------------+------------+-----+\n|window                                    |IP          |count|\n+------------------------------------------+------------+-----+\n|[2016-12-06 08:59:20, 2016-12-06 08:59:50]|120.52.73.97|985  |\n|[2016-12-06 08:59:20, 2016-12-06 08:59:50]|120.52.73.98|752  |\n|[2016-12-06 08:59:20, 2016-12-06 08:59:50]|97.77.104.22|514  |\n+------------------------------------------+------------+-----+\n\n+------------------------------------------+--------------+-----+\n|window                                    |IP            |count|\n+------------------------------------------+--------------+-----+\n|[2016-12-06 08:59:30, 2016-12-06 09:00:00]|185.28.193.95 |6    |\n|[2016-12-06 08:59:30, 2016-12-06 09:00:00]|178.22.148.122|4    |\n|[2016-12-06 08:59:30, 2016-12-06 09:00:00]|120.52.73.97  |4    |\n+------------------------------------------+--------------+-----+\n\n+------------------------------------------+-------------+-----+\n|window                                    |IP           |count|\n+------------------------------------------+-------------+-----+\n|[2016-12-06 08:59:30, 2016-12-06 09:00:00]|185.28.193.95|939  |\n|[2016-12-06 08:59:30, 2016-12-06 09:00:00]|120.52.73.97 |524  |\n|[2016-12-06 08:59:30, 2016-12-06 09:00:00]|120.52.73.98 |429  |\n+------------------------------------------+-------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Example 5\n\nList the top-3 IP sources with more accesses in the last 30 seconds. Update the list every 10 seconds.\n\nPrint the country of the URL, assuming there is a CSV with the country for each IP.\n\nPrint also the total number of requests for each IP overtime.\n","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ndef dumpBatchDF(df, epoch_id):\n    df.show(20, False)\n  \nspark = SparkSession \\\n    .builder \\\n    .appName(\"StructuredWebLogExample\") \\\n    .getOrCreate()\n\n# Read the countries file\nuserSchema = StructType().add(\"IP\", \"string\").add(\"country\", \"string\")\ncountries = spark.read.schema(userSchema).csv(\"countries.csv\")\n\n# Create DataFrame representing the stream of input \n# lines from connection to logsender 7776\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"logsender\") \\\n    .option(\"port\", 7777) \\\n    .load() \n\nsplit_lines = split(lines['value'], ' ')\nlines = lines.withColumn('time', split_lines.getItem(0).cast(\"timestamp\")) \\\n    .withColumn('IP', split_lines.getItem(1).cast(\"string\")) \\\n    .withColumn('code', split_lines.getItem(2).cast(\"integer\")) \\\n    .withColumn('op', split_lines.getItem(3).cast(\"string\")) \\\n    .withColumn('URL', split_lines.getItem(4).cast(\"string\")) \\\n    .withColumn('dur', split_lines.getItem(5).cast(\"float\")) \\\n    .drop('value')\n\nquery = lines.groupBy(window(lines.time,\"30 seconds\",\"10 seconds\"),'IP') \\\n    .agg(count('*').alias('count')) \\\n    .orderBy('window','count', ascending=False) \\\n    .limit(3)\n\nquery = query.join(countries, query.IP == countries.IP, \"inner\")\n\n# Just dump the created data frames\nquery = query \\\n    .writeStream \\\n    .outputMode(\"complete\") \\\n    .foreachBatch(dumpBatchDF) \\\n    .start()\n\nquery.awaitTermination(60)\nquery.stop()\n","metadata":{},"execution_count":null,"outputs":[]}]}